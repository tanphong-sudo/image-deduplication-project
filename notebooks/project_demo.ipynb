{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanphong-sudo/image-deduplication-project/blob/main/notebooks/project_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zBmZM_nNyVgS",
        "outputId": "ea932b82-0248-4b9c-baeb-d1e3a5f3dd35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'image-deduplication-project'...\n",
            "remote: Enumerating objects: 293, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/32)\u001b[K\rremote: Counting objects:   6% (2/32)\u001b[K\rremote: Counting objects:   9% (3/32)\u001b[K\rremote: Counting objects:  12% (4/32)\u001b[K\rremote: Counting objects:  15% (5/32)\u001b[K\rremote: Counting objects:  18% (6/32)\u001b[K\rremote: Counting objects:  21% (7/32)\u001b[K\rremote: Counting objects:  25% (8/32)\u001b[K\rremote: Counting objects:  28% (9/32)\u001b[K\rremote: Counting objects:  31% (10/32)\u001b[K\rremote: Counting objects:  34% (11/32)\u001b[K\rremote: Counting objects:  37% (12/32)\u001b[K\rremote: Counting objects:  40% (13/32)\u001b[K\rremote: Counting objects:  43% (14/32)\u001b[K\rremote: Counting objects:  46% (15/32)\u001b[K\rremote: Counting objects:  50% (16/32)\u001b[K\rremote: Counting objects:  53% (17/32)\u001b[K\rremote: Counting objects:  56% (18/32)\u001b[K\rremote: Counting objects:  59% (19/32)\u001b[K\rremote: Counting objects:  62% (20/32)\u001b[K\rremote: Counting objects:  65% (21/32)\u001b[K\rremote: Counting objects:  68% (22/32)\u001b[K\rremote: Counting objects:  71% (23/32)\u001b[K\rremote: Counting objects:  75% (24/32)\u001b[K\rremote: Counting objects:  78% (25/32)\u001b[K\rremote: Counting objects:  81% (26/32)\u001b[K\rremote: Counting objects:  84% (27/32)\u001b[K\rremote: Counting objects:  87% (28/32)\u001b[K\rremote: Counting objects:  90% (29/32)\u001b[K\rremote: Counting objects:  93% (30/32)\u001b[K\rremote: Counting objects:  96% (31/32)\u001b[K\rremote: Counting objects: 100% (32/32)\u001b[K\rremote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 293 (delta 3), reused 5 (delta 1), pack-reused 261 (from 1)\u001b[K\n",
            "Receiving objects: 100% (293/293), 117.45 KiB | 4.70 MiB/s, done.\n",
            "Resolving deltas: 100% (150/150), done.\n",
            "/content/image-deduplication-project/image-deduplication-project/image-deduplication-project/image-deduplication-project\n"
          ]
        }
      ],
      "source": [
        "# Clone repository\n",
        "GIT_REPO_URL = \"https://github.com/tanphong-sudo/image-deduplication-project\"\n",
        "!git clone $GIT_REPO_URL\n",
        "%cd image-deduplication-project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ab1iD7lEa-na"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00EjQ45la-nb"
      },
      "source": [
        "**Note:** The C++ SimHash module is optional. If build fails, you can still use FAISS and MinHash methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "u5wybkgGa-ni",
        "outputId": "b45e1faf-beb5-4669-c6b3-40b89506d2e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/image-deduplication-project/image-deduplication-project/image-deduplication-project/image-deduplication-project/src/lsh_cpp_module\n",
            "‚úì Pybind11 found: /usr/local/lib/python3.12/dist-packages/pybind11/include\n",
            "running build_ext\n",
            "building 'lsh_cpp_module' extension\n",
            "creating build/temp.linux-x86_64-cpython-312/lsh_cpp\n",
            "x86_64-linux-gnu-g++ -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.12/dist-packages/pybind11/include -Ilsh_cpp -I/usr/include/python3.12 -c lsh_cpp/bindings.cpp -o build/temp.linux-x86_64-cpython-312/lsh_cpp/bindings.o -DVERSION_INFO=\\\"1.0.0\\\" -std=c++14 -O3 -ffast-math -Wall -Wextra -march=native -fopenmp\n",
            "creating build/lib.linux-x86_64-cpython-312\n",
            "x86_64-linux-gnu-g++ -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-312/lsh_cpp/bindings.o -L/usr/lib/x86_64-linux-gnu -o build/lib.linux-x86_64-cpython-312/lsh_cpp_module.cpython-312-x86_64-linux-gnu.so -fopenmp\n",
            "copying build/lib.linux-x86_64-cpython-312/lsh_cpp_module.cpython-312-x86_64-linux-gnu.so -> \n",
            "/content/image-deduplication-project/image-deduplication-project/image-deduplication-project/image-deduplication-project\n"
          ]
        }
      ],
      "source": [
        "# Build C++ SimHash module\n",
        "%cd src/lsh_cpp_module\n",
        "\n",
        "# Verify pybind11 is installed\n",
        "try:\n",
        "    import pybind11\n",
        "    print(f\"‚úì Pybind11 found: {pybind11.get_include()}\")\n",
        "except ImportError:\n",
        "    print(\"Installing pybind11...\")\n",
        "    !pip install -q pybind11\n",
        "\n",
        "# Build the module\n",
        "!python setup.py build_ext --inplace\n",
        "\n",
        "%cd ../.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwmG3iv3a-nj"
      },
      "source": [
        "---\n",
        "\n",
        "## üì• Prepare Your Dataset\n",
        "\n",
        "Choose one of the methods below to provide images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cMTnkNb_a-nj",
        "outputId": "c214d4c2-b00d-4825-aed0-656232152b02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Copying 720 images from Drive...\n",
            "‚úì Copied 720 images to data/raw/\n"
          ]
        }
      ],
      "source": [
        "# Option 1: Upload from Google Drive (Colab only)\n",
        "# 1. Upload your images to Google Drive folder\n",
        "# 2. Run this cell and authorize\n",
        "# 3. Your images will be copied to data/raw/\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Check if running on Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"‚ö†Ô∏è This cell is for Google Colab only\")\n",
        "    print(\"üí° If running locally, your images should already be in data/raw/\")\n",
        "    print(\"   Skip this cell and continue to Quick Start\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Specify your Drive folder path (edit this!)\n",
        "    DRIVE_FOLDER = '/content/drive/MyDrive/DSA/Project/data/raw'  # <- Change this to your folder\n",
        "\n",
        "    # Copy images to data/raw/\n",
        "    os.makedirs('data/raw', exist_ok=True)\n",
        "\n",
        "    if os.path.exists(DRIVE_FOLDER):\n",
        "        image_files = [f for f in os.listdir(DRIVE_FOLDER)\n",
        "                       if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
        "\n",
        "        print(f\"Copying {len(image_files)} images from Drive...\")\n",
        "        for img_file in image_files:\n",
        "            src = os.path.join(DRIVE_FOLDER, img_file)\n",
        "            dst = os.path.join('data/raw', img_file)\n",
        "            shutil.copy2(src, dst)\n",
        "\n",
        "        print(f\"‚úì Copied {len(image_files)} images to data/raw/\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Folder not found: {DRIVE_FOLDER}\")\n",
        "        print(\"Please edit DRIVE_FOLDER path in the cell above\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKg0JufBa-nk"
      },
      "source": [
        "### Option 2: Upload Files Directly\n",
        "\n",
        "Use Colab's file upload widget:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI0rMoZ4a-nk"
      },
      "outputs": [],
      "source": [
        "# Option 2: Upload files directly (Colab only)\n",
        "import os\n",
        "\n",
        "# Check if running on Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"‚ö†Ô∏è This cell is for Google Colab only\")\n",
        "    print(\"üí° If running locally, place your images in data/raw/ folder\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    os.makedirs('data/raw', exist_ok=True)\n",
        "\n",
        "    print(\"Click 'Choose Files' and select your images...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Save uploaded files to data/raw/\n",
        "    for filename, content in uploaded.items():\n",
        "        with open(f'data/raw/{filename}', 'wb') as f:\n",
        "            f.write(content)\n",
        "\n",
        "    print(f\"\\n‚úì Uploaded {len(uploaded)} images to data/raw/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfBsM-j_a-nk"
      },
      "source": [
        "### Option 3: Download from URL\n",
        "\n",
        "If your dataset is hosted somewhere (Dropbox, OneDrive, etc.):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjp0IbNPa-nk"
      },
      "outputs": [],
      "source": [
        "# Option 3: Download dataset from URL (zip file)\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Example: Download and extract a zip file\n",
        "DATASET_URL = \"https://your-url.com/dataset.zip\"  # <- Change this to your URL\n",
        "\n",
        "os.makedirs('data/raw', exist_ok=True)\n",
        "\n",
        "# Uncomment below to download\n",
        "# !wget -q {DATASET_URL} -O dataset.zip\n",
        "# with zipfile.ZipFile('dataset.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('data/raw')\n",
        "# !rm dataset.zip\n",
        "# print(\"‚úì Downloaded and extracted dataset\")\n",
        "\n",
        "print(\"Edit DATASET_URL and uncomment the code above\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJl5XQZfa-nl"
      },
      "source": [
        "---\n",
        "\n",
        "## ‚ö° Quick Start (Recommended)\n",
        "\n",
        "Run this cell for default configuration (EfficientNet + FAISS):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jZ2UxccOa-nl",
        "outputId": "0578badf-1f1e-4a59-9d06-7d8c5362513f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üñºÔ∏è  IMAGE DEDUPLICATION PIPELINE\n",
            "======================================================================\n",
            "üìÅ Dataset:            data/raw\n",
            "üìä Feature Extractor:  efficientnet\n",
            "üîç Search Method:      faiss\n",
            "üìè Threshold:          50.0\n",
            "üíæ Output Directory:   data/processed\n",
            "======================================================================\n",
            "\n",
            "2025-11-03 15:54:57,333 INFO Found 720 images\n",
            "2025-11-03 15:54:57,336 INFO Extracted 10 unique labels.\n",
            "2025-11-03 15:54:57,340 INFO Saved image paths and labels to data/processed/image_labels.csv\n",
            "2025-11-03 15:54:57,386 INFO Saved 25560 ground-truth pairs to data/processed/ground_truth_pairs.json\n",
            "2025-11-03 15:54:57,386 INFO Using extractor=efficientnet device=cpu\n",
            "2025-11-03 15:55:38,310 INFO Features loaded: N=720, d=1280\n",
            "2025-11-03 15:55:38,310 INFO Building FAISS index type=flat nlist=1024\n",
            "2025-11-03 15:55:38,311 INFO Running kNN self-search (FAISS)\n",
            "2025-11-03 15:55:38,842 INFO Saved clusters (count=10) to faiss_clusters.json\n",
            "2025-11-03 15:55:38,842 INFO Copied representative for cluster 0: obj10__340.png\n",
            "2025-11-03 15:55:38,842 INFO Copied representative for cluster 1: obj1__125.png\n",
            "2025-11-03 15:55:38,843 INFO Copied representative for cluster 2: obj2__15.png\n",
            "2025-11-03 15:55:38,843 INFO Copied representative for cluster 3: obj3__160.png\n",
            "2025-11-03 15:55:38,843 INFO Copied representative for cluster 4: obj4__145.png\n",
            "2025-11-03 15:55:38,843 INFO Copied representative for cluster 5: obj5__345.png\n",
            "2025-11-03 15:55:38,844 INFO Copied representative for cluster 6: obj6__330.png\n",
            "2025-11-03 15:55:38,844 INFO Copied representative for cluster 7: obj7__330.png\n",
            "2025-11-03 15:55:38,844 INFO Copied representative for cluster 8: obj8__155.png\n",
            "2025-11-03 15:55:38,844 INFO Copied representative for cluster 9: obj9__320.png\n",
            "2025-11-03 15:55:38,876 INFO Full evaluation saved to data/processed/evaluation_full.json\n",
            "2025-11-03 15:55:38,876 INFO Pipeline finished.\n",
            "\n",
            "======================================================================\n",
            "üìä IMAGE DEDUPLICATION - EVALUATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "üéØ CLUSTERING METRICS\n",
            "----------------------------------------------------------------------\n",
            "  Precision:        1.0000 (100.00%)\n",
            "  Recall:           1.0000 (100.00%)\n",
            "  True Positives:   25,560\n",
            "  False Positives:  0\n",
            "  False Negatives:  0\n",
            "\n",
            "üì¶ CLUSTERS\n",
            "----------------------------------------------------------------------\n",
            "  Total clusters: 10\n",
            "    Cluster 0: obj10 -  72 images\n",
            "    Cluster 1: obj1  -  72 images\n",
            "    Cluster 2: obj2  -  72 images\n",
            "    Cluster 3: obj3  -  72 images\n",
            "    Cluster 4: obj4  -  72 images\n",
            "    Cluster 5: obj5  -  72 images\n",
            "    Cluster 6: obj6  -  72 images\n",
            "    Cluster 7: obj7  -  72 images\n",
            "    Cluster 8: obj8  -  72 images\n",
            "    Cluster 9: obj9  -  72 images\n",
            "\n",
            "‚≠ê REPRESENTATIVES\n",
            "----------------------------------------------------------------------\n",
            "    Cluster 0: obj10__340.png\n",
            "    Cluster 1: obj1__125.png\n",
            "    Cluster 2: obj2__15.png\n",
            "    Cluster 3: obj3__160.png\n",
            "    Cluster 4: obj4__145.png\n",
            "    Cluster 5: obj5__345.png\n",
            "    Cluster 6: obj6__330.png\n",
            "    Cluster 7: obj7__330.png\n",
            "    Cluster 8: obj8__155.png\n",
            "    Cluster 9: obj9__320.png\n",
            "\n",
            "‚è±Ô∏è  PERFORMANCE\n",
            "----------------------------------------------------------------------\n",
            "  feature_extraction       :  38.2678s ( 99.9%)\n",
            "  faiss_search             :   0.0199s (  0.1%)\n",
            "  TOTAL                    :  38.2877s\n",
            "\n",
            "üíæ MEMORY USAGE\n",
            "----------------------------------------------------------------------\n",
            "  feature_extraction       :  1020.05 MB\n",
            "  faiss_search             :   871.93 MB\n",
            "  PEAK TOTAL               :  1891.98 MB\n",
            "\n",
            "======================================================================\n",
            "‚úÖ Evaluation complete!\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run with best defaults: EfficientNet + FAISS\n",
        "!python run_pipeline.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_2BsPD7a-nl"
      },
      "source": [
        "---\n",
        "\n",
        "## üíæ Feature Caching (Optional but Recommended)\n",
        "\n",
        "**Why cache features?**\n",
        "- Feature extraction takes too long (one-time cost)\n",
        "- After caching, you can test different search methods in <1 second\n",
        "\n",
        "**Extract and save features once:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "stCHC5soa-nl",
        "outputId": "5f678677-752f-497d-8bc3-f11cc149928a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üñºÔ∏è  IMAGE DEDUPLICATION PIPELINE\n",
            "======================================================================\n",
            "üìÅ Dataset:            data/raw\n",
            "üìä Feature Extractor:  efficientnet\n",
            "üîç Search Method:      faiss\n",
            "üìè Threshold:          50.0\n",
            "üíæ Output Directory:   data/processed\n",
            "======================================================================\n",
            "\n",
            "2025-11-03 15:57:14,056 INFO Found 720 images\n",
            "2025-11-03 15:57:14,059 INFO Extracted 10 unique labels.\n",
            "2025-11-03 15:57:14,062 INFO Saved image paths and labels to data/processed/image_labels.csv\n",
            "2025-11-03 15:57:14,063 INFO Ground-truth file already exists at data/processed/ground_truth_pairs.json, skipping generation.\n",
            "2025-11-03 15:57:14,063 INFO Using extractor=efficientnet device=cpu\n",
            "2025-11-03 15:57:54,360 INFO Features loaded: N=720, d=1280\n",
            "2025-11-03 15:57:54,360 INFO Building FAISS index type=flat nlist=1024\n",
            "2025-11-03 15:57:54,362 INFO Running kNN self-search (FAISS)\n",
            "2025-11-03 15:57:54,985 INFO Saved clusters (count=10) to faiss_clusters.json\n",
            "2025-11-03 15:57:54,985 INFO Copied representative for cluster 0: obj10__340.png\n",
            "2025-11-03 15:57:54,986 INFO Copied representative for cluster 1: obj1__125.png\n",
            "2025-11-03 15:57:54,986 INFO Copied representative for cluster 2: obj2__15.png\n",
            "2025-11-03 15:57:54,986 INFO Copied representative for cluster 3: obj3__160.png\n",
            "2025-11-03 15:57:54,987 INFO Copied representative for cluster 4: obj4__145.png\n",
            "2025-11-03 15:57:54,987 INFO Copied representative for cluster 5: obj5__345.png\n",
            "2025-11-03 15:57:54,987 INFO Copied representative for cluster 6: obj6__330.png\n",
            "2025-11-03 15:57:54,987 INFO Copied representative for cluster 7: obj7__330.png\n",
            "2025-11-03 15:57:54,988 INFO Copied representative for cluster 8: obj8__155.png\n",
            "2025-11-03 15:57:54,988 INFO Copied representative for cluster 9: obj9__320.png\n",
            "2025-11-03 15:57:55,042 INFO Full evaluation saved to data/processed/evaluation_full.json\n",
            "2025-11-03 15:57:55,042 INFO Pipeline finished.\n",
            "\n",
            "======================================================================\n",
            "üìä IMAGE DEDUPLICATION - EVALUATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "üéØ CLUSTERING METRICS\n",
            "----------------------------------------------------------------------\n",
            "  Precision:        1.0000 (100.00%)\n",
            "  Recall:           1.0000 (100.00%)\n",
            "  True Positives:   25,560\n",
            "  False Positives:  0\n",
            "  False Negatives:  0\n",
            "\n",
            "üì¶ CLUSTERS\n",
            "----------------------------------------------------------------------\n",
            "  Total clusters: 10\n",
            "    Cluster 0: obj10 -  72 images\n",
            "    Cluster 1: obj1  -  72 images\n",
            "    Cluster 2: obj2  -  72 images\n",
            "    Cluster 3: obj3  -  72 images\n",
            "    Cluster 4: obj4  -  72 images\n",
            "    Cluster 5: obj5  -  72 images\n",
            "    Cluster 6: obj6  -  72 images\n",
            "    Cluster 7: obj7  -  72 images\n",
            "    Cluster 8: obj8  -  72 images\n",
            "    Cluster 9: obj9  -  72 images\n",
            "\n",
            "‚≠ê REPRESENTATIVES\n",
            "----------------------------------------------------------------------\n",
            "    Cluster 0: obj10__340.png\n",
            "    Cluster 1: obj1__125.png\n",
            "    Cluster 2: obj2__15.png\n",
            "    Cluster 3: obj3__160.png\n",
            "    Cluster 4: obj4__145.png\n",
            "    Cluster 5: obj5__345.png\n",
            "    Cluster 6: obj6__330.png\n",
            "    Cluster 7: obj7__330.png\n",
            "    Cluster 8: obj8__155.png\n",
            "    Cluster 9: obj9__320.png\n",
            "\n",
            "‚è±Ô∏è  PERFORMANCE\n",
            "----------------------------------------------------------------------\n",
            "  feature_extraction       :  37.4487s ( 99.9%)\n",
            "  faiss_search             :   0.0211s (  0.1%)\n",
            "  TOTAL                    :  37.4697s\n",
            "\n",
            "üíæ MEMORY USAGE\n",
            "----------------------------------------------------------------------\n",
            "  feature_extraction       :  1015.52 MB\n",
            "  faiss_search             :   832.18 MB\n",
            "  PEAK TOTAL               :  1847.70 MB\n",
            "\n",
            "======================================================================\n",
            "‚úÖ Evaluation complete!\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python run_pipeline.py --save-features data/processed/features.npy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_Sp3gpVa-nl"
      },
      "source": [
        "**Now test different methods instantly (no re-extraction needed):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "64BE7_o3a-nm",
        "outputId": "840b94f2-c952-478f-fe45-d5cd19907db1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üñºÔ∏è  IMAGE DEDUPLICATION PIPELINE\n",
            "======================================================================\n",
            "üìÅ Dataset:            data/raw\n",
            "üìä Feature Extractor:  efficientnet\n",
            "üîç Search Method:      simhash\n",
            "üìè Threshold:          50.0\n",
            "üî¢ Hamming Threshold:  5\n",
            "üíæ Output Directory:   data/processed\n",
            "======================================================================\n",
            "\n",
            "2025-11-03 15:59:00,366 INFO Found 720 images\n",
            "2025-11-03 15:59:00,369 INFO Extracted 10 unique labels.\n",
            "2025-11-03 15:59:00,374 INFO Saved image paths and labels to data/processed/image_labels.csv\n",
            "2025-11-03 15:59:00,374 INFO Ground-truth file already exists at data/processed/ground_truth_pairs.json, skipping generation.\n",
            "2025-11-03 15:59:00,374 INFO Loading features from data/processed/features.npy\n",
            "2025-11-03 15:59:00,378 INFO Features loaded: N=720, d=1280\n",
            "2025-11-03 15:59:00,381 INFO Using SimHashSearch (C++ backend via Pybind11)\n",
            "‚úì Using C++ SimHash LSH (dim=1280, bits=64, tables=8)\n",
            "2025-11-03 15:59:00,483 INFO Running SimHash search (tables=8, hamming_threshold=5)...\n",
            "2025-11-03 15:59:00,665 INFO Clustering with threshold=50.0\n",
            "2025-11-03 15:59:01,162 INFO Saved simhash clusters (12)\n",
            "2025-11-03 15:59:01,162 INFO Copied representative for cluster 0: obj10__340.png\n",
            "2025-11-03 15:59:01,163 INFO Copied representative for cluster 1: obj1__125.png\n",
            "2025-11-03 15:59:01,163 INFO Copied representative for cluster 2: obj2__15.png\n",
            "2025-11-03 15:59:01,163 INFO Copied representative for cluster 3: obj3__160.png\n",
            "2025-11-03 15:59:01,164 INFO Copied representative for cluster 4: obj4__145.png\n",
            "2025-11-03 15:59:01,164 INFO Copied representative for cluster 5: obj5__345.png\n",
            "2025-11-03 15:59:01,164 INFO Copied representative for cluster 6: obj6__330.png\n",
            "2025-11-03 15:59:01,164 INFO Copied representative for cluster 7: obj7__330.png\n",
            "2025-11-03 15:59:01,165 INFO Copied representative for cluster 8: obj8__155.png\n",
            "2025-11-03 15:59:01,165 INFO Copied representative for cluster 9: obj8__105.png\n",
            "2025-11-03 15:59:01,165 INFO Copied representative for cluster 10: obj8__80.png\n",
            "2025-11-03 15:59:01,165 INFO Copied representative for cluster 11: obj9__320.png\n",
            "2025-11-03 15:59:01,240 INFO Full evaluation saved to data/processed/evaluation_full.json\n",
            "2025-11-03 15:59:01,240 INFO Pipeline finished.\n",
            "\n",
            "======================================================================\n",
            "üìä IMAGE DEDUPLICATION - EVALUATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "üéØ CLUSTERING METRICS\n",
            "----------------------------------------------------------------------\n",
            "  Precision:        1.0000 (100.00%)\n",
            "  Recall:           0.9843 (98.43%)\n",
            "  True Positives:   25,159\n",
            "  False Positives:  0\n",
            "  False Negatives:  401\n",
            "\n",
            "üì¶ CLUSTERS\n",
            "----------------------------------------------------------------------\n",
            "  Total clusters: 10\n",
            "    Cluster 0: obj10 -  72 images\n",
            "    Cluster 1: obj1  -  72 images\n",
            "    Cluster 2: obj2  -  72 images\n",
            "    Cluster 3: obj3  -  72 images\n",
            "    Cluster 4: obj4  -  72 images\n",
            "    Cluster 5: obj5  -  72 images\n",
            "    Cluster 6: obj6  -  72 images\n",
            "    Cluster 7: obj7  -  72 images\n",
            "    Cluster 8: obj8  -  72 images\n",
            "    Cluster 9: obj9  -  72 images\n",
            "\n",
            "‚≠ê REPRESENTATIVES\n",
            "----------------------------------------------------------------------\n",
            "    Cluster 0: obj10__340.png\n",
            "    Cluster 1: obj1__125.png\n",
            "    Cluster 2: obj2__15.png\n",
            "    Cluster 3: obj3__160.png\n",
            "    Cluster 4: obj4__145.png\n",
            "    Cluster 5: obj5__345.png\n",
            "    Cluster 6: obj6__330.png\n",
            "    Cluster 7: obj7__330.png\n",
            "    Cluster 8: obj8__155.png\n",
            "    Cluster 9: obj9__320.png\n",
            "\n",
            "‚è±Ô∏è  PERFORMANCE\n",
            "----------------------------------------------------------------------\n",
            "  feature_extraction       :   0.0000s (  0.0%)\n",
            "  simhash_search           :   0.1814s (100.0%)\n",
            "  TOTAL                    :   0.1814s\n",
            "\n",
            "üíæ MEMORY USAGE\n",
            "----------------------------------------------------------------------\n",
            "  feature_extraction       :     0.00 MB\n",
            "  simhash_search           :   158.72 MB\n",
            "  PEAK TOTAL               :   158.72 MB\n",
            "\n",
            "======================================================================\n",
            "‚úÖ Evaluation complete!\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python run_pipeline.py --load-features data/processed/features.npy --method simhash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSSCPNHDa-nm"
      },
      "source": [
        "---\n",
        "\n",
        "## üéõÔ∏è Advanced: Custom Configuration\n",
        "\n",
        "Want to try different combinations? Edit and run this cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrJ5na7ka-nm"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CONFIGURATION - Edit these parameters\n",
        "# ========================================\n",
        "\n",
        "# Feature Extractor: 'resnet' or 'efficientnet'\n",
        "EXTRACTOR = 'efficientnet'\n",
        "\n",
        "# Search Method: 'faiss', 'simhash', or 'minhash'\n",
        "METHOD = 'faiss'\n",
        "\n",
        "# Threshold (for FAISS and MinHash): recommended 50-100 for FAISS, 0.5 for MinHash\n",
        "THRESHOLD = 50.0\n",
        "\n",
        "# Hamming Threshold (for SimHash only): recommended 5 for EfficientNet, 6 for ResNet\n",
        "HAMMING_THRESHOLD = 5\n",
        "\n",
        "USE_CACHE = False  # Change to True after running once\n",
        "\n",
        "# ========================================\n",
        "# Run pipeline with your configuration\n",
        "# ========================================\n",
        "import os\n",
        "\n",
        "if USE_CACHE and os.path.exists('data/processed/features.npy'):\n",
        "    if METHOD == 'simhash':\n",
        "        !python run_pipeline.py --load-features data/processed/features.npy --method {METHOD} --hamming-threshold {HAMMING_THRESHOLD}\n",
        "    else:\n",
        "        !python run_pipeline.py --load-features data/processed/features.npy --method {METHOD} --threshold {THRESHOLD}\n",
        "else:\n",
        "    if METHOD == 'simhash':\n",
        "        !python run_pipeline.py --extractor {EXTRACTOR} --method {METHOD} --hamming-threshold {HAMMING_THRESHOLD}\n",
        "    else:\n",
        "        !python run_pipeline.py --extractor {EXTRACTOR} --method {METHOD} --threshold {THRESHOLD}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JKFM8jga-nm"
      },
      "source": [
        "### üí° Recommended Configurations\n",
        "\n",
        "| Configuration | Extractor | Method | Threshold/Hamming | Use Case |\n",
        "|--------------|-----------|--------|-------------------|----------|\n",
        "| **Best Accuracy** ‚≠ê | efficientnet | faiss | 50 | Production, guaranteed results |\n",
        "| **Best Speed** ‚≠ê | efficientnet | faiss | 50 | Fast queries, moderate dataset |\n",
        "| **Baseline** | efficientnet | minhash | 0.5 | Comparison benchmark |\n",
        "| **Large Scale** üîß | efficientnet | simhash | 5 | Billions of images (requires C++ build) |\n",
        "| **High Dimensional** üîß | resnet | simhash | 6 | Very high-dim features (requires C++ build) |\n",
        "\n",
        "‚≠ê = Works on all systems  \n",
        "üîß = Requires C++ SimHash module\n",
        "\n",
        "*Edit the configuration cell above to try different combinations!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "echXprxJa-nm"
      },
      "source": [
        "## üìä View Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiowKoKOa-nm"
      },
      "outputs": [],
      "source": [
        "# Load and display evaluation results\n",
        "import json\n",
        "\n",
        "with open('data/processed/evaluation_full.json', 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(json.dumps(results, indent=2))\n",
        "\n",
        "# Visualize duplicate clusters\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Visualizing duplicate clusters...\")\n",
        "print(\"=\" * 60)\n",
        "!python view_results.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxPj3A96a-nn"
      },
      "source": [
        "## ‚ö° Benchmark: C++ vs Python SimHash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeJRppcIa-nn"
      },
      "outputs": [],
      "source": [
        "# Run performance benchmark\n",
        "%cd src/lsh_cpp_module\n",
        "!python benchmark_comparison.py\n",
        "%cd ../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTIIss-Za-nn"
      },
      "outputs": [],
      "source": [
        "# Display benchmark visualization\n",
        "from IPython.display import Image, display\n",
        "display(Image('src/lsh_cpp_module/lsh_benchmark_results.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xPKeSS5a-nn"
      },
      "source": [
        "## üìà Summary & Conclusions\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **FAISS**: Best overall performance with 100% accuracy and fastest query time\n",
        "2. **SimHash LSH (C++)**: Excellent for large-scale applications, memory efficient\n",
        "3. **MinHash LSH**: Good baseline, suitable for set-based similarity\n",
        "\n",
        "### Performance Highlights:\n",
        "\n",
        "- **C++ SimHash** achieves 165-187x speedup over Python implementations\n",
        "- **Multi-probing** critical for high recall with deep learning features\n",
        "- **Hamming threshold tuning**: 5-6 optimal for 100% recall\n",
        "\n",
        "### Recommendations:\n",
        "\n",
        "- Use **FAISS** for production systems requiring guaranteed accuracy\n",
        "- Use **SimHash C++** for billion-scale datasets with memory constraints\n",
        "- Tune `hamming_threshold` based on feature extractor (ResNet50: 6, EfficientNet: 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFxt2etXa-nn"
      },
      "source": [
        "---\n",
        "\n",
        "## üìö References\n",
        "\n",
        "1.\n",
        "---\n",
        "\n",
        "**Team**: L√™ B·∫£o T·∫•n Phong, Nguy·ªÖn Anh Qu√¢n, Ph·∫°m VƒÉn H√™n  \n",
        "**HCMUT** - Data Structures & Algorithms - 2025\n",
        "\n",
        "‚≠ê [GitHub Repository](https://github.com/tanphong-sudo/image-deduplication-project)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}